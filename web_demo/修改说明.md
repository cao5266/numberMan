# Web Demo 目录修改详细说明

本文档详细说明在 `web_demo` 目录下所做的所有修改，包括API地址替换、文本转语音流程等。

## 目录结构

```
web_demo/
├── server.py                    # 普通模式服务器（非实时ASR）
├── server_realtime.py           # 实时ASR模式服务器
├── voiceapi/
│   ├── llm.py                   # 大语言模型接口（API地址配置）
│   ├── tts.py                   # 文本转语音模块
│   └── asr.py                   # 语音识别模块
└── static/
    ├── js/
    │   ├── dialog.js            # 普通模式对话框逻辑
    │   ├── dialog_realtime.js   # 实时模式对话框逻辑
    │   └── MiniLive2.js         # 视频处理逻辑
    └── html/
        ├── dialog.html          # 普通模式对话框页面
        └── dialog_RealTime.html # 实时模式对话框页面
```

---

## 1. API地址替换

### 1.1 `voiceapi/llm.py` - 大模型API配置

**修改内容：**
- 将豆包（字节跳动）API替换为DeepSeek API
- 更新了 `base_url` 和 `api_key`
- 更新了 `model_name`

**具体修改：**

```python
# 修改前（注释掉的豆包配置）
# base_url = "https://ark.cn-beijing.volces.com/api/v3"
# api_key = ""
# model_name = "doubao-pro-32k-character-241215"

# 修改后（DeepSeek配置）
base_url = "https://api.deepseek.com"
api_key = "sk-7bdc0a3467194ad8890328f0a2d23e22"
model_name = "deepseek-chat"
```

**功能说明：**
- `llm_stream(prompt)` 函数：使用OpenAI兼容接口调用DeepSeek API
- 返回流式响应，支持实时文本生成
- 使用系统提示词："你是人工智能助手"

**注意事项：**
- ⚠️ **重要**：`api_key` 需要替换为您自己的DeepSeek API密钥
- 当前使用的是测试密钥，生产环境必须更换

---

## 2. 文本转语音（TTS）流程

### 2.1 `voiceapi/tts.py` - TTS引擎管理

**主要功能：**
- 使用 `sherpa-onnx` 离线TTS引擎
- 支持多种TTS模型：`sherpa-onnx-vits-zh-ll`、`vits-zh-hf-theresa`、`vits-melo-tts-zh_en`
- 支持多音色（voice_id）和语速（voice_speed）调节
- 返回Base64编码的WAV音频数据

**关键函数：**

1. **`get_audio(text, voice_speed=1.0, voice_id=0, target_sample_rate=16000)`**
   - 异步函数，将文本转换为音频
   - 参数：
     - `text`: 要转换的文本
     - `voice_speed`: 语速（默认1.0）
     - `voice_id`: 音色ID（默认0）
     - `target_sample_rate`: 目标采样率（默认16000Hz）
   - 返回：Base64编码的WAV音频字符串

2. **`TTSEngineManager` 类**
   - 单例模式，全局管理TTS引擎
   - `initialize(args)`: 初始化TTS引擎
   - `get_engine()`: 获取TTS引擎实例

**支持的TTS模型配置：**
- `sherpa-onnx-vits-zh-ll`: 采样率16000Hz（默认）
- `vits-zh-hf-theresa`: 采样率22050Hz
- `vits-melo-tts-zh_en`: 采样率44100Hz

---

### 2.2 `server.py` - 普通模式服务器

**主要修改：**

1. **导入TTS模块**
```python
try:
    from voiceapi.tts import get_audio as tts_get_audio, TTSEngineManager
    USE_REAL_TTS = True
except ImportError as e:
    USE_REAL_TTS = False
```

2. **TTS引擎初始化**
   - 自动查找models目录
   - 初始化TTS引擎管理器
   - 如果模型不存在，使用模拟音频（fallback）

3. **`get_audio()` 函数**
   - 处理voice_id和voice_speed参数（支持字符串和数字）
   - 调用真实TTS引擎生成音频
   - 如果TTS失败，使用fallback音频文件

4. **`gen_stream()` 函数 - 核心流式处理**
   - 接收LLM的流式文本输出
   - 按标点符号切分句子（至少15个字符）
   - 对每个句子调用 `get_audio()` 生成音频
   - 实时返回JSON格式的数据块：
     ```json
     {
         "text": "生成的文本",
         "audio": "Base64编码的音频数据",
         "endpoint": false  // 是否结束
     }
     ```

5. **标点符号检测**
   - 支持的标点：`。？！；…，、()（）`
   - 如果累积文本超过60字符还没标点，强制输出前50字符
   - 处理剩余文本，确保所有文本都被转换为音频

**数据流程：**
```
用户输入 → LLM流式生成文本 → 检测标点符号 → 切分句子 
→ TTS生成音频 → 返回JSON数据块 → 前端播放音频
```

---

### 2.3 `server_realtime.py` - 实时ASR模式服务器

**主要修改：**

1. **WebSocket ASR支持**
   - `/asr` 端点：WebSocket连接，实时语音识别
   - 使用 `sherpa-onnx` 进行实时ASR
   - 支持VAD（语音活动检测）

2. **生命周期管理**
   - 服务启动时初始化ASR和TTS引擎
   - 服务关闭时清理资源

3. **`gen_stream()` 函数**
   - 与 `server.py` 类似，但针对实时模式优化
   - 标点符号检测从第8个字符开始（更早触发TTS）
   - 支持实时语音输入 → ASR → LLM → TTS → 音频输出

**支持的ASR模型：**
- `zipformer-bilingual`: 中英双语流式识别
- `sensevoice`: 多语言离线识别
- `paraformer-trilingual`: 三语言识别
- `paraformer-en`: 英语识别
- `whisper-*`: Whisper系列模型

---

## 3. 前端JavaScript修改

### 3.1 `static/js/dialog.js` - 普通模式对话框

**主要修改：**

1. **服务器地址配置**
```javascript
let server_url = "http://localhost:8888/eb_stream"
```

2. **`sendTextMessage()` 函数**
   - 发送文本消息到服务器
   - 接收SSE（Server-Sent Events）流式响应
   - 解析JSON数据块，提取文本和音频
   - 将音频数据加入播放队列

3. **`sendAudioMessage()` 函数**
   - 发送音频消息（录音）
   - 将音频转换为Base64
   - 接收SSE流式响应
   - 处理ASR识别的文本（`data.prompt`）

4. **音频播放队列**
   - `audioQueue`: 存储待播放的音频数据
   - `playAudio()`: 顺序播放音频队列
   - 使用Web Audio API解码和播放音频
   - 同时将音频数据传递给WebAssembly模块（用于口型同步）

5. **音色选择**
   - 从父窗口获取 `voiceDropdown` 的值
   - 将音色ID传递给服务器

**数据流程：**
```
用户输入（文本/音频） → 发送到服务器 → 接收SSE流 
→ 解析JSON → 提取文本和音频 → 显示文本 → 播放音频
```

---

### 3.2 `static/js/dialog_realtime.js` - 实时模式对话框

**主要修改：**

1. **WebSocket连接**
```javascript
let websocket_url = "ws://localhost:8888/asr?samplerate=16000"
```

2. **实时录音**
   - 使用 `PCMAudioRecorder` 进行PCM录音
   - VAD检测：检测到人声时开始发送音频数据
   - 静音检测：800ms无声音后自动结束

3. **ASR实时识别**
   - WebSocket接收ASR识别结果
   - 实时更新识别文本（`data.text`）
   - 识别完成后（`data.idx == -1`）触发LLM请求

4. **`handleResponseStream()` 函数**
   - 处理SSE流式响应
   - 解析JSON数据块
   - 提取文本和音频数据
   - 加入播放队列

5. **`start_new_round()` 函数**
   - 开始新一轮对话
   - 重置ASR状态
   - 启动录音和WebSocket连接

**数据流程：**
```
用户说话 → VAD检测 → PCM录音 → WebSocket发送 
→ ASR识别 → 实时显示文本 → 识别完成 → LLM生成 
→ TTS转换 → 音频播放
```

---

### 3.3 `static/js/MiniLive2.js` - 视频处理

**主要功能：**
- 处理视频帧和3D模型渲染
- 与音频同步进行口型动画
- 支持iOS 17+的特殊处理

**关键修改：**
- iOS 17+使用Blob格式存储视频帧（内存优化）
- 其他系统使用ImageBitmap
- 视频帧逆序播放（循环动画）
- 支持角色选择（`characterDropdown`）
- 角色切换时重新加载视频和模型数据

**角色选择功能：**
```javascript
characterDropdown.addEventListener('change', async function() {
    isPaused = true;
    asset_dir = this.value;  // 获取选中的角色目录
    await videoProcessor.init(asset_dir + "/01.mp4", asset_dir + "/combined_data.json.gz");
    await loadCombinedData();
    await setupVertsBuffers();
    isPaused = false;
    await processVideoFrames();
});
```

---

### 3.4 `static/js/audio_recorder.js` - PCM音频录音器

**主要功能：**
- 使用AudioWorklet进行PCM音频录制
- 支持16kHz采样率（与ASR模型匹配）
- 实时输出Int16格式的PCM数据

**关键特性：**
- `PCMAudioRecorder` 类：封装录音功能
- `connect(audioCallback)`: 连接音频流，开始录音
- `stop()`: 停止录音并清理资源
- AudioWorklet处理器：将浮点音频转换为Int16 PCM

**使用示例：**
```javascript
const recorder = new PCMAudioRecorder();
await recorder.connect(async (pcmData) => {
    // pcmData 是 Int16Array 格式的PCM数据
    // 可以实时发送到WebSocket或处理
    ws.send(pcmData.buffer);
});
```

---

## 4. HTML页面修改

### 4.1 `static/dialog.html` - 普通模式对话框

**主要特点：**
- 支持文本和语音两种输入模式
- 切换按钮：键盘图标 ↔ 麦克风图标
- 语音模式：点击说话，录音后发送
- 文本模式：输入框 + 发送按钮

**样式：**
- 聊天容器：上半部分显示虚拟形象，下半部分显示对话
- 消息样式：用户消息（蓝色，右对齐），AI消息（灰色，左对齐）

---

### 4.2 `static/dialog_RealTime.html` - 实时模式对话框

**主要特点：**
- 实时语音识别（无需点击停止）
- VAD自动检测说话开始和结束
- 实时显示识别文本
- 引入 `audio_recorder.js` 进行PCM录音

**样式修改：**
- 添加了 `user-select: none` 防止文本选择
- 语音输入区域：`点击开始对话`

**脚本引入：**
```html
<script src="js/audio_recorder.js"></script>
<script src="js/dialog_realtime.js"></script>
```

---

### 4.3 `static/MiniLive_RealTime.html` - 实时模式主页面

**主要特点：**
- 包含角色选择器（`characterDropdown`）
- 包含音色选择器（`voiceDropdown`）
- 嵌入对话框iframe（`dialog_RealTime.html`）
- 显示虚拟形象视频

**音色选择器配置：**
- 使用数字ID（0-4）
- 5种音色：温柔女、温柔男、甜美女、青年女、磁性男

---

### 4.4 `static/MiniLive_new.html` - 新版本主页面

**主要特点：**
- 包含角色选择器（`characterDropdown`）
- 包含音色选择器（`voiceDropdown`）
- 嵌入对话框iframe（`dialog.html`）
- 显示虚拟形象视频

**音色选择器配置：**
- 使用字符串ID（模型名称）
- 4种音色：青涩男、霸气男、妩媚女、甜美女
- ⚠️ 注意：字符串ID需要TTS模型支持名称映射

---

## 5. 完整数据流程

### 5.1 普通模式（文本输入）

```
1. 用户输入文本
   ↓
2. 前端：sendTextMessage() 发送POST请求到 /eb_stream
   ↓
3. 服务器：server.py → gen_stream()
   ↓
4. LLM：llm_stream() 流式生成文本
   ↓
5. 服务器：检测标点符号，切分句子
   ↓
6. TTS：get_audio() 生成音频（Base64）
   ↓
7. 服务器：返回JSON数据块（SSE流）
   ↓
8. 前端：接收数据，显示文本，播放音频
   ↓
9. 口型同步：音频数据传递给WebAssembly模块
```

### 5.2 普通模式（语音输入）

```
1. 用户录音
   ↓
2. 前端：sendAudioMessage() 发送音频（Base64）
   ↓
3. 服务器：call_asr_api() 识别语音（当前为模拟）
   ↓
4. 后续流程与文本输入相同（步骤3-9）
```

### 5.3 实时模式

```
1. 用户说话
   ↓
2. VAD检测到人声 → 开始录音
   ↓
3. PCM音频数据 → WebSocket发送到 /asr
   ↓
4. ASR实时识别 → 返回识别文本
   ↓
5. 前端实时显示识别文本
   ↓
6. VAD检测到静音（800ms）→ 结束识别
   ↓
7. 识别完成 → 触发LLM请求
   ↓
8. 后续流程与普通模式相同（LLM → TTS → 播放）
```

---

## 6. 配置说明

### 6.1 LLM API配置

**文件：** `web_demo/voiceapi/llm.py`

```python
# DeepSeek API配置
base_url = "https://api.deepseek.com"
api_key = "你的API密钥"  # ⚠️ 需要替换
model_name = "deepseek-chat"
```

**获取API密钥：**
1. 访问 https://www.deepseek.com/
2. 注册账号并获取API密钥
3. 替换 `api_key` 变量

---

### 6.2 TTS模型配置

**文件：** `web_demo/server.py` 或 `web_demo/server_realtime.py`

**模型目录结构：**
```
web_demo/models/
└── sherpa-onnx-vits-zh-ll/
    ├── model.onnx
    ├── lexicon.txt
    ├── dict/
    └── tokens.txt
```

**启动参数（server_realtime.py）：**
```bash
python server_realtime.py \
    --models-root ./models \
    --tts-model sherpa-onnx-vits-zh-ll \
    --tts-provider cpu \
    --asr-model zipformer-bilingual \
    --asr-provider cpu \
    --threads 2
```

---

### 6.3 ASR模型配置

**支持的模型：**
- `zipformer-bilingual`: 中英双语（推荐，实时识别）
- `sensevoice`: 多语言（中文、英文、日文、韩文、粤语）
- `paraformer-trilingual`: 三语言
- `paraformer-en`: 英语
- `whisper-medium`: Whisper模型

**模型目录：**
```
web_demo/models/
├── sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20/
│   ├── encoder-epoch-99-avg-1.onnx
│   ├── decoder-epoch-99-avg-1.onnx
│   ├── joiner-epoch-99-avg-1.onnx
│   └── tokens.txt
└── silero_vad/  # VAD模型（用于离线ASR）
    └── silero_vad.onnx
```

---

### 6.4 音色选择器配置

**文件位置：**
- `web_demo/static/MiniLive_RealTime.html` - 实时模式页面
- `web_demo/static/MiniLive_new.html` - 新版本页面

**音色配置说明：**

1. **实时模式页面 (`MiniLive_RealTime.html`)**
   - 使用数字ID（0-4）
   - 配置示例：
     ```html
     <select id="voiceDropdown">
         <option value=0>温柔女</option>
         <option value=1>温柔男</option>
         <option value=2>甜美女</option>
         <option value=3>青年女</option>
         <option value=4>磁性男</option>
     </select>
     ```
   - JavaScript处理：`dialog_realtime.js` 中会解析为整数
     ```javascript
     const parsedValue = parseInt(voiceDropdown.value, 10);
     ```

2. **新版本页面 (`MiniLive_new.html`)**
   - 使用字符串ID（模型名称）
   - 配置示例：
     ```html
     <select id="voiceDropdown">
         <option value="male-qn-qingse">青涩男</option>
         <option value="male-qn-badao">霸气男</option>
         <option value="wumei_yujie">妩媚女</option>
         <option value="female-tianmei">甜美女</option>
     </select>
     ```
   - ⚠️ **注意**：如果使用字符串ID，需要确保TTS模型支持该音色名称

**音色ID传递流程：**

1. **前端获取音色ID**
   ```javascript
   // dialog.js / dialog_realtime.js
   const voiceDropdown = window.parent.document.getElementById('voiceDropdown');
   let characterName = voiceDropdown.value;
   ```

2. **发送到服务器**
   ```javascript
   // 请求体
   {
       "input_mode": "text",
       "prompt": "用户输入",
       "voice_id": characterName,  // 音色ID
       "voice_speed": ""           // 语速（可选）
   }
   ```

3. **服务器处理**
   ```python
   # server.py / server_realtime.py
   # 处理voice_id：可能是数字字符串、数字、或空字符串
   if voice_id == "" or voice_id is None:
       voice_id_int = 0
   elif isinstance(voice_id, str):
       try:
           voice_id_int = int(voice_id) if voice_id.isdigit() else 0
       except:
           voice_id_int = 0
   else:
       voice_id_int = int(voice_id) if voice_id else 0
   ```

4. **TTS生成音频**
   ```python
   # voiceapi/tts.py
   base64_string = await get_audio(text, voice_speed=voice_speed_float, voice_id=voice_id_int)
   ```

**音色ID与TTS模型的对应关系：**
- 不同的TTS模型支持不同的音色ID范围
- `sherpa-onnx-vits-zh-ll` 通常支持 0-4 的音色ID
- 具体支持的音色数量和ID范围取决于模型文件

**注意事项：**
- ⚠️ 如果音色ID超出模型支持范围，TTS可能会失败或使用默认音色
- ⚠️ 字符串类型的音色ID需要TTS模型支持名称映射
- ⚠️ 建议统一使用数字ID（0-4），确保兼容性

---

## 7. 关键参数说明

### 7.1 TTS参数

- **voice_id**: 音色ID（整数，默认0）
  - 支持范围：取决于TTS模型（通常0-4）
  - 前端传递：数字字符串或整数
  - 服务器处理：自动转换为整数，失败则使用0
- **voice_speed**: 语速（浮点数，默认1.0，范围0.5-2.0）
  - 前端传递：字符串或数字
  - 服务器处理：自动转换为浮点数，失败则使用1.0
- **target_sample_rate**: 目标采样率（默认16000Hz）
  - 与ASR模型采样率匹配
  - 如果TTS模型采样率不同，会自动重采样

### 7.2 ASR参数

- **samplerate**: 采样率（默认16000Hz）
- **VAD_SILENCE_DURATION**: 静音检测时长（800ms）
- **rule1_min_trailing_silence**: 最小尾随静音（2.4秒）
- **rule2_min_trailing_silence**: 最小尾随静音（1.2秒）

### 7.3 LLM参数

- **stream**: 流式响应（True）
- **system prompt**: "你是人工智能助手"
- **temperature**: 默认（未设置）

---

## 8. 错误处理

### 8.1 TTS失败处理

- 如果TTS引擎初始化失败，使用模拟音频文件
- 模拟音频路径：`web_demo/static/common/test.wav`
- 日志输出：`⚠️ 使用模拟音频（固定音频文件）`

### 8.2 LLM失败处理

- 如果LLM调用失败，返回错误消息
- 错误消息也会通过TTS转换为音频
- 日志输出：`❌ 大模型调用失败: {error}`

### 8.3 ASR失败处理

- 如果ASR引擎初始化失败，服务无法启动
- WebSocket连接失败会重试
- 日志输出：`asr: engine loaded in {time}s`

---

## 9. 性能优化

### 9.1 流式处理

- LLM流式输出，减少延迟
- 按标点符号切分，提前生成音频
- 音频队列播放，实现边生成边播放

### 9.2 内存管理

- iOS 17+使用Blob格式，减少内存占用
- 及时释放视频帧和音频缓冲区
- WebAssembly内存管理

### 9.3 并发处理

- 异步处理TTS和ASR
- 使用线程池执行同步操作
- WebSocket和SSE并发处理

---

## 10. 测试建议

### 10.1 测试步骤

1. **启动服务器**
   ```bash
   # 普通模式
   python web_demo/server.py
   
   # 实时模式
   python web_demo/server_realtime.py --port 8888
   ```

2. **测试文本输入**
   - 打开浏览器访问页面
   - 切换到文本模式
   - 输入文本，查看响应

3. **测试语音输入**
   - 切换到语音模式
   - 点击说话，录音后发送
   - 查看识别结果和回复

4. **测试实时模式**
   - 使用实时模式页面
   - 说话，查看实时识别
   - 查看LLM回复和音频播放

### 10.2 常见问题

1. **TTS没有声音**
   - 检查模型文件是否存在
   - 查看服务器日志
   - 确认采样率配置正确

2. **ASR识别不准确**
   - 检查麦克风权限
   - 调整VAD参数
   - 尝试不同的ASR模型

3. **LLM没有回复**
   - 检查API密钥是否正确
   - 查看网络连接
   - 检查服务器日志

4. **音色不生效**
   - 检查音色ID是否在模型支持范围内
   - 确认TTS模型文件完整
   - 查看服务器日志中的TTS调用信息

5. **实时ASR不工作**
   - 检查WebSocket连接是否建立
   - 确认麦克风权限已授予
   - 检查VAD模型文件是否存在
   - 查看浏览器控制台和服务器日志

---

## 11. 总结

### 11.1 主要修改点

1. ✅ **API地址替换**：豆包 → DeepSeek
2. ✅ **TTS集成**：文本 → 音频转换
3. ✅ **流式处理**：实时生成和播放
4. ✅ **实时ASR**：WebSocket语音识别
5. ✅ **前端优化**：音频队列和播放管理

### 11.2 技术栈

- **后端**：FastAPI, OpenAI API, sherpa-onnx
- **前端**：JavaScript, Web Audio API, WebSocket, SSE
- **AI模型**：DeepSeek LLM, sherpa-onnx TTS/ASR

### 11.3 后续优化建议

1. 添加更多TTS音色选择
2. 优化音频质量（采样率、比特率）
3. 添加情绪识别和情感TTS
4. 支持多语言切换
5. 添加对话历史管理
6. 优化移动端体验

---

## 12. 文件修改清单

### 12.1 后端文件

- ✅ `web_demo/server.py` - 普通模式服务器
- ✅ `web_demo/server_realtime.py` - 实时模式服务器
- ✅ `web_demo/voiceapi/llm.py` - LLM API配置
- ✅ `web_demo/voiceapi/tts.py` - TTS模块（未修改，使用现有）
- ✅ `web_demo/voiceapi/asr.py` - ASR模块（未修改，使用现有）

### 12.2 前端文件

- ✅ `web_demo/static/js/dialog.js` - 普通模式对话框
- ✅ `web_demo/static/js/dialog_realtime.js` - 实时模式对话框
- ✅ `web_demo/static/js/MiniLive2.js` - 视频处理（iOS优化）
- ✅ `web_demo/static/js/audio_recorder.js` - PCM音频录音器（实时模式）
- ✅ `web_demo/static/dialog.html` - 普通模式页面
- ✅ `web_demo/static/dialog_RealTime.html` - 实时模式页面
- ✅ `web_demo/static/MiniLive_RealTime.html` - 实时模式主页面（含音色选择器）
- ✅ `web_demo/static/MiniLive_new.html` - 新版本主页面（含音色选择器）

### 12.3 配置文件

- ✅ `web_demo/voiceapi/llm.py` - API密钥配置

---

---

## 13. 快速参考

### 13.1 关键文件路径

| 文件 | 路径 | 说明 |
|------|------|------|
| LLM配置 | `web_demo/voiceapi/llm.py` | API密钥和模型配置 |
| 普通服务器 | `web_demo/server.py` | 普通模式服务器 |
| 实时服务器 | `web_demo/server_realtime.py` | 实时ASR模式服务器 |
| TTS模块 | `web_demo/voiceapi/tts.py` | 文本转语音 |
| ASR模块 | `web_demo/voiceapi/asr.py` | 语音识别 |
| 普通对话框 | `web_demo/static/js/dialog.js` | 普通模式前端逻辑 |
| 实时对话框 | `web_demo/static/js/dialog_realtime.js` | 实时模式前端逻辑 |
| 音频录音 | `web_demo/static/js/audio_recorder.js` | PCM录音器 |

### 13.2 API端点

| 端点 | 方法 | 说明 |
|------|------|------|
| `/eb_stream` | POST | 文本/音频输入，返回SSE流 |
| `/asr` | WebSocket | 实时语音识别（仅实时模式） |
| `/static/*` | GET | 静态文件服务 |

### 13.3 请求/响应格式

**请求格式（/eb_stream）：**
```json
{
    "input_mode": "text" | "audio",
    "prompt": "用户输入的文本",
    "audio": "Base64编码的音频（audio模式）",
    "voice_id": 0,
    "voice_speed": 1.0
}
```

**响应格式（SSE流）：**
```json
{
    "text": "生成的文本",
    "audio": "Base64编码的WAV音频",
    "endpoint": false
}
```

**WebSocket消息格式（/asr）：**
- 发送：PCM音频数据（二进制）
- 接收：
```json
{
    "text": "识别的文本",
    "finished": false,
    "idx": 0
}
```

### 13.4 环境变量和配置

| 配置项 | 位置 | 说明 |
|--------|------|------|
| API密钥 | `voiceapi/llm.py` | DeepSeek API密钥 |
| 服务器端口 | `server.py` / `server_realtime.py` | 默认8888 |
| 模型目录 | 命令行参数 | `--models-root` |
| TTS模型 | 命令行参数 | `--tts-model` |
| ASR模型 | 命令行参数 | `--asr-model` |

### 13.5 常用命令

**启动普通模式服务器：**
```bash
cd web_demo
python server.py
```

**启动实时模式服务器：**
```bash
cd web_demo
python server_realtime.py --port 8888 --models-root ./models
```

**查看日志：**
- 服务器日志：控制台输出
- 前端日志：浏览器控制台（F12）

### 13.6 故障排查检查清单

- [ ] API密钥是否配置正确
- [ ] 模型文件是否存在
- [ ] 服务器是否正常启动
- [ ] 浏览器控制台是否有错误
- [ ] 网络连接是否正常
- [ ] 麦克风权限是否授予
- [ ] 端口是否被占用
- [ ] 依赖包是否安装完整

---

**文档版本：** 1.1  
**最后更新：** 2024年  
**维护者：** 开发团队

